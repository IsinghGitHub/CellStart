{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TTS_V2_using_Tacotron_2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPClP9xnMY7aLlMd7s5v2NG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsinghGitHub/CellStrat/blob/master/TTS_V2_using_Tacotron_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz4g-jFPf-Jy",
        "colab_type": "text"
      },
      "source": [
        "**Tacotron-2:**\n",
        "\n",
        "Tensorflow implementation of DeepMind's Tacotron-2. A deep neural network architecture described in this paper: [Natural TTS synthesis by conditioning Wavenet on MEL spectogram predictions](https://arxiv.org/pdf/1712.05884.pdf)\n",
        "\n",
        "This Repository contains additional improvements and attempts over the paper, we thus propose paper_hparams.py file which holds the exact hyperparameters to reproduce the paper results without any additional extras.\n",
        "\n",
        "Suggested hparams.py file which is default in use, contains the hyperparameters with extras that proved to provide better results in most cases. Feel free to toy with the parameters as needed.\n",
        "\n",
        "DIFFERENCES WILL BE HIGHLIGHTED IN DOCUMENTATION SHORTLY."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTQE2GTYgtip",
        "colab_type": "text"
      },
      "source": [
        "**Steps to run the code:**\n",
        "\n",
        "Step (0): Get your dataset, here I have set the examples of Ljspeech, en_US and en_UK (from M-AILABS).\n",
        "\n",
        "Step (1): Preprocess your data. This will give you the training_data folder.\n",
        "\n",
        "Step (2): Train your Tacotron model. Yields the logs-Tacotron folder.\n",
        "\n",
        "Step (3): Synthesize/Evaluate the Tacotron model. Gives the tacotron_output folder.\n",
        "\n",
        "Step (4): Train your Wavenet model. Yield the logs-Wavenet folder.\n",
        "\n",
        "Step (5): Synthesize audio using the Wavenet model. Gives the wavenet_output folder.\n",
        "\n",
        "Note: Steps 2, 3, and 4 can be made with a simple run for both Tacotron and WaveNet (Tacotron-2, step ( * ))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCkYgCRZrNcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get update -y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCF_W4MOrSyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y libasound-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i51Rb9btsv52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get update"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "467XrLeL-Ds4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8duY3c--8Lp",
        "colab_type": "text"
      },
      "source": [
        "Changing the directory to Tacotron directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvXvZ7X0N4gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from os.path import exists, join, expanduser\n",
        "\n",
        "os.chdir('/content/drive/My Drive/CellStrat/TTS/Tacotron-2') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsPYJM86-qBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MBh63zpPbWN",
        "colab_type": "text"
      },
      "source": [
        "## **Ignore the above error for any requirements not matched for now**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8beJLvpnC5Di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMXGhMbcL4YC",
        "colab_type": "text"
      },
      "source": [
        "Download and extract the LJSpeech Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txnUsrZlL90L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xvf LJSpeech-1.1.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpRQgX5nRwum",
        "colab_type": "text"
      },
      "source": [
        "`**Preprocessing**`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKzm4Wac-Gi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python preprocess.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY_NxUtnSUs1",
        "colab_type": "text"
      },
      "source": [
        "## Training:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFx1xJCmHbmZ",
        "colab_type": "text"
      },
      "source": [
        "### To train both models sequentially (one after the other):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dreteAJSLrQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python train.py --model='Tacotron-2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yIbPzS1Hh7X",
        "colab_type": "text"
      },
      "source": [
        "### Feature prediction model can separately be trained using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm7epzuZxFyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python train.py --model='Tacotron'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNRaTmc6Hnui",
        "colab_type": "text"
      },
      "source": [
        "### checkpoints will be made each 5000 steps and stored under logs-Tacotron folder.\n",
        "\n",
        "Naturally, training the wavenet separately is done by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ue3Koc22wSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python train.py --model='WaveNet'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENDU7FuxHwop",
        "colab_type": "text"
      },
      "source": [
        "### logs will be stored inside logs-Wavenet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nc7WddwH9sC",
        "colab_type": "text"
      },
      "source": [
        "Note:\n",
        "\n",
        "\n",
        "1.   ### If model argument is not provided, training will default to Tacotron-2 model training. (both models)\n",
        "\n",
        "\n",
        "2.  ### Please refer to train arguments under train.py for a set of options you can use.\n",
        "\n",
        "3.   ### It is now possible to make wavenet preprocessing alone using wavenet_proprocess.py."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMUG1Tz_ItDh",
        "colab_type": "text"
      },
      "source": [
        "## Synthesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmCJhr18IwlI",
        "colab_type": "text"
      },
      "source": [
        "To synthesize audio in an End-to-End (text to audio) manner (both models at work):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVr_Btdy361m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python synthesize.py --model='Tacotron-2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaBHYPGRI3k-",
        "colab_type": "text"
      },
      "source": [
        "For the spectrogram prediction network (separately), there are three types of mel spectrograms synthesis:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjAc4zCFJCJU",
        "colab_type": "text"
      },
      "source": [
        "Evaluation (synthesis on custom sentences). This is what we'll usually use after having a full end to end model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhMVciapI_dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python synthesize.py --model='Tacotron'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z81wphcHJMwe",
        "colab_type": "text"
      },
      "source": [
        "Natural synthesis (let the model make predictions alone by feeding last decoder output to the next time step).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgvjlI98JUMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python synthesize.py --model='Tacotron' --mode='synthesis' --GTA=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgRjTUnHJaMk",
        "colab_type": "text"
      },
      "source": [
        "Ground Truth Aligned synthesis (DEFAULT: the model is assisted by true labels in a teacher forcing manner). This synthesis method is used when predicting mel spectrograms used to train the wavenet vocoder. (yields better results as stated in the paper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCJVw1EQJb_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python synthesize.py --model='Tacotron' --mode='synthesis' --GTA=True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqex_o_LJk5o",
        "colab_type": "text"
      },
      "source": [
        "Synthesizing the waveforms conditionned on previously synthesized Mel-spectrograms (separately) can be done with:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R33IuQzAJkXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python synthesize.py --model='WaveNet'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_sGSsMgJuMy",
        "colab_type": "text"
      },
      "source": [
        "Note:\n",
        "\n",
        "If model argument is not provided, synthesis will default to Tacotron-2 model synthesis. (End-to-End TTS)\n",
        "Please refer to synthesis arguments under synthesize.py for a set of options you can use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sexdpK8J1VQ",
        "colab_type": "text"
      },
      "source": [
        "## References and Resources:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPnuFE6pJ3fF",
        "colab_type": "text"
      },
      "source": [
        "Natural TTS synthesis by conditioning Wavenet on MEL \n",
        "\n",
        "spectogram predictions\n",
        "\n",
        "Original tacotron paper\n",
        "\n",
        "Attention-Based Models for Speech Recognition\n",
        "\n",
        "Wavenet: A generative model for raw audio\n",
        "\n",
        "Fast Wavenet\n",
        "\n",
        "r9y9/wavenet_vocoder\n",
        "\n",
        "keithito/tacotron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg3QpYZ1Jvyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}